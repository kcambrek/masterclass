{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masterclass Unsupervised analysis \n",
    "## Hands on session 2 - Clustering algorithms\n",
    "#### Rijkswaterstaat | Datalab | E. Taskesen | Oct 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the process of grouping samples which is then used to determine the \"natural\" or \"data driven\" groupings in the data set. If clusters emerge, their properties can be summarized. Although various clustering methods can produce a partitioning of the samples in the data set, different methods yields in different groupings since each implicitly imposes a structure on the data.\n",
    "\n",
    "In this notebook we will generate data sets, and experiment with various clustering algorithms, such as hierarchical, k-means, DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><b>Clustering summary:</b></center>\n",
    "<img src=\"./img/cluster_info.png\",width=750,height=750>\n",
    "\n",
    "\n",
    "<center>http://scikit-learn.org/stable/modules/clustering.html</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hierarchical clustering</h2>\n",
    "<h3>The algorithm</h3>\n",
    "\n",
    "Hierarchical clustering groups data over a variety of scales by creating a cluster tree or dendrogram. The tree is not a single set of clusters, but rather a multilevel hierarchy, where clusters at one level are joined as clusters at the next level. This allows you to decide the level or scale of clustering that is most appropriate for your application.\n",
    "\n",
    "<br>\n",
    "Hierarchical clustering is based on three major steps:\n",
    "1.\tFinding the similarity or dissimilarity between every pair of objects in the data set.  In this step, you calculate the distance between objects.\n",
    "2.\tGrouping the objects into a binary, hierarchical cluster tree (dendrogram).  In this step, you link pairs of objects that are in close proximity (have a high similarity) using different linkage methods. The linkage method uses the distance information generated in step 1 to determine the similarity of objects to each other. As objects are paired into binary clusters, the newly formed clusters are grouped into larger clusters until a hierarchical tree is formed\n",
    "3.\tDetermine where to cut the hierarchical tree into clusters. In this step the obtained dendrogram is analyzed in order to decide which cutting level best suits the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dendrogram â€“ Linkage methods\n",
    "Once the dissimilarity between objects in the data set has been computed, you can determine how objects in the data set should be grouped into clusters by choosing one of the linkage methods.\n",
    "\n",
    "Python function dendrogram takes the distance information generated by dist and links pairs of objects that are close together into binary clusters. The linkage function then links these newly formed clusters to each other and to other objects to create bigger clusters until all objects in the original data set are linked together in a hierarchical tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../src/\"))\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_circles\n",
    "from scipy.cluster.hierarchy import inconsistent\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from clusteval import clusteval\n",
    "from silhouette import silhouette\n",
    "from dbindex import dbindex\n",
    "from scatter import scatter\n",
    "from DBSCAN import DBSCAN\n",
    "from HDBSCAN import HDBSCAN\n",
    "from dendrogram_fancy import dendrogram_fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MESSY data.\n",
    "\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close together.\n",
    "[X, labx] = make_blobs(n_samples=500, n_features=2, centers=3, random_state=4)\n",
    "\n",
    "# Scatter the dots\n",
    "out=scatter(X[:,0],X[:,1], labx=[], labx_type='unique', width=8, height=8, title='Messy data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2\n",
    "What is the optimal number of clusters based on the scatter plot?\n",
    "\n",
    "a) 2 \n",
    "<br>\n",
    "b) 10\n",
    "<br>\n",
    "c) 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: Scatter with the real labx\n",
    "out=scatter(X[:,0],X[:,1], labx=labx, labx_type='unique', width=8, height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3\n",
    "\n",
    "Inspect the dendrogram with respect to the distances calculated earlier. Can you reconstruct the distances between the data points from dendrogram? The purpose of this exercise is to get you acquainted with clustering gain insight in the different parameters that you can set.\n",
    "\n",
    "##### Complete link method\n",
    "This method defines the distance between two groups as the distance between their two farthest-apart members. This method usually yields clusters that are well separated and compact.\n",
    "\n",
    "##### Average link method\n",
    "This algorithm defines the distance between groups as the average distance between each of the members, weighted so that the two groups have an equal influence on the final result. \n",
    "\n",
    "##### Single link method\n",
    "The distance between two groups is defined as the distance between their two closest members.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the dendrogram to find the optimal number of clusters\n",
    "# 'complete'\n",
    "# 'average'\n",
    "# 'single'\n",
    "# 'ward'\n",
    "Z=linkage(X, method='complete', metric='euclidean')\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "dendrogram(Z,leaf_font_size=12)\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()\n",
    "\n",
    "# On the x axis you see labels. If you don't specify anything else they are the indices of your samples in X.\n",
    "# On the y axis you see the distances (of the 'complete' method in our case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 4\n",
    "The messy dataset shows three noisy clusters that are almost overlapping. Which linkage works best to obtain the two underlying noisy clusters?\n",
    "\n",
    "a) single\n",
    "<br>\n",
    "b) complete\n",
    "<br>\n",
    "c) average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 5\n",
    "Hierarchical clustering produces a dendrogram. To give the desired number of clusters, the tree can be cut at a desired horizontal level. The number of vertical stems of the dendrogram intersected by the horizontal line, corresponds to the number of clusters. A scatterplot of the samples, colour-coded by cluster membership, reveals the clustering (note that when there are more than two features in the dataset, the clustering is performed taking all features into account, but that only the first two features are displayed). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=linkage(X, method='complete', metric='euclidean')\n",
    "max_d=7\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "out=dendrogram_fancy(\n",
    "    Z,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=max_d,\n",
    "    max_d=max_d,  # plot a horizontal cut-off line\n",
    ")\n",
    "\n",
    "labxNew = fcluster(Z, max_d, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25, 10))\n",
    "# out=dendrogram_fancy(\n",
    "#    Z,\n",
    "#    truncate_mode='lastp',\n",
    "#    p=12,\n",
    "#    leaf_rotation=90.,\n",
    "#    leaf_font_size=12.,\n",
    "#    show_contracted=True,\n",
    "#    annotate_above=10,\n",
    "#    max_d=10,  # plot a horizontal cut-off line\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 6\n",
    "If you cut the euclidean distance dendogram at 5.2 How many clusters do you obtain?\n",
    "\n",
    "a) 3\n",
    "<br>\n",
    "b) 4\n",
    "<br>\n",
    "c) 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "labxNew = fcluster(Z, 5.2, criterion='distance')\n",
    "print(\"Number of clusters: %d\" %len(np.unique(labxNew)))\n",
    "out=scatter(X[:,0],X[:,1], labx=labxNew, labx_type='unique', width=8, height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 7\n",
    "Explain why selecting two clusters does not work to obtain the two underlying noisy clusters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER:\n",
    "k=2\n",
    "labxNew = fcluster(Z, k, criterion='maxclust')\n",
    "out=scatter(X[:,0],X[:,1], labx=labxNew, labx_type='unique', width=10, height=10, title='Exercise 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 8\n",
    "If you have outliers that you want to remove, which linkage type would you use to select and remove those?\n",
    "\n",
    "a) Average\n",
    "<br>\n",
    "b) Complete\n",
    "<br>\n",
    "c) Single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 9\n",
    "Hierarchical clustering is also great in <b>non-spherical</b> data.\n",
    "How would you cluster the underneath data? Suppose you want to capture the inner and seperately the outer circle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-spherical data.\n",
    "[X, labx] = make_circles(n_samples=1000, factor=0.3, noise=0.1, random_state=4)\n",
    "# Plot the data distribution. (Here's another way to plot scatter graph)\n",
    "out=scatter(X[:,0],X[:,1], labx=[], labx_type='unique', width=8, height=8, title='Circles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LINKAGE TYPE\n",
    "Z=linkage(X, method='single', metric='euclidean')\n",
    "#Z=linkage(X, method='complete', metric='euclidean')\n",
    "#Z=linkage(X, method='average', metric='euclidean')\n",
    "\n",
    "max_d = 0.2\n",
    "plt.figure(figsize=(8, 4))\n",
    "out=dendrogram_fancy(\n",
    "    Z,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=max_d,\n",
    "    max_d=max_d,  # plot a horizontal cut-off line\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER: WITH LINKAGE TYPE\n",
    "labxNew = fcluster(Z, 2, criterion='maxclust')\n",
    "out=scatter(X[:,0],X[:,1], labx=labxNew, labx_type='unique', width=8, height=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
